{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 143856,
     "status": "ok",
     "timestamp": 1600752667533,
     "user": {
      "displayName": "Juliana Damurie",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgflBzYptWKQ0dbJgh2lCPGPTscmRPFiDOL4HXGBw=s64",
      "userId": "11274300898498138619"
     },
     "user_tz": -120
    },
    "id": "9khrJsim2CKP",
    "outputId": "8bd27879-1627-435b-e2f3-7c49b46d499a"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 172703,
     "status": "ok",
     "timestamp": 1600752696523,
     "user": {
      "displayName": "Juliana Damurie",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgflBzYptWKQ0dbJgh2lCPGPTscmRPFiDOL4HXGBw=s64",
      "userId": "11274300898498138619"
     },
     "user_tz": -120
    },
    "id": "synyQK0s2qd7",
    "outputId": "b36aa9a6-7e89-4cf0-8699-2e1c4bf6cc5d"
   },
   "outputs": [],
   "source": [
    "!pip install keras\n",
    "\n",
    "!pip install kerassurgeon\n",
    "\n",
    "! pip install tfkerassurgeon\n",
    "#Selection of version of tensorflow in colab\n",
    "%tensorflow_version 1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 182812,
     "status": "ok",
     "timestamp": 1600752706828,
     "user": {
      "displayName": "Juliana Damurie",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgflBzYptWKQ0dbJgh2lCPGPTscmRPFiDOL4HXGBw=s64",
      "userId": "11274300898498138619"
     },
     "user_tz": -120
    },
    "id": "YGODOuhj--zC",
    "outputId": "4e8ee807-fa2c-4208-bf28-fe74ae18a360"
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Aug 29 10:06:29 2019\n",
    "\n",
    "@author: gourgue\n",
    "\"\"\"\n",
    "\n",
    "import keras\n",
    "#%%\n",
    "from keras.applications import DenseNet201\n",
    "from keras              import backend, layers, models, utils\n",
    "from keras.layers       import Input,Dense, Activation, Flatten, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers       import Dropout\n",
    "from keras.models       import Model, load_model\n",
    "from keras.optimizers   import Adam\n",
    "from keras.callbacks    import ModelCheckpoint, CSVLogger\n",
    "#from keras.utils        import np_utils, plot_model\n",
    "\n",
    "#from fonction_compteur import ouvrir\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import time, datetime, os, random, sys\n",
    "\n",
    "# import kerassurgeon\n",
    "# from kerassurgeon.operations import delete_layer, insert_layer, delete_channels\n",
    "# from kerassurgeon import Surgeon\n",
    "\n",
    "sys.path.append('/content/drive/My Drive/Stage/code_test_nicolas/')\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils   import class_weight\n",
    "from my_classes import DataGenerator, get_labels, DataGeneratorTopHat\n",
    "from function_model import create_model, SaveModel, LoadModel\n",
    "from function_verification import creation_folder\n",
    "from function_data import save_data\n",
    "from function_affiche import  plot_time\n",
    "\n",
    "from skimage.io import imread, imsave\n",
    "\n",
    "import scipy.io.matlab as mio\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 16})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1754380,
     "status": "ok",
     "timestamp": 1600782245001,
     "user": {
      "displayName": "Juliana Damurie",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgflBzYptWKQ0dbJgh2lCPGPTscmRPFiDOL4HXGBw=s64",
      "userId": "11274300898498138619"
     },
     "user_tz": -120
    },
    "id": "j03w-ZzAbckn",
    "outputId": "9f3d2225-c59c-4cb5-f830-b7fc91d05851"
   },
   "outputs": [],
   "source": [
    "\n",
    "#%% parameters non reglable\n",
    "debut=time.time()\n",
    "classe_name=['saines','infecté']\n",
    "datasets=['basic','different','ponderate','combo']\n",
    "pre_trains=['imagenet',None]\n",
    "colors=['R','G','B','RGB']\n",
    "champs=['RAW_0','BF','DF',\"RAW\"]\n",
    "patients=['CAT01/','KPJ0/','DA/','LE/']\n",
    "nb_led=[1,7,19,35,49]\n",
    "BF3=list(range(0+1,nb_led[0]+1))+list(range(nb_led[0]+1,nb_led[1]+1))\n",
    "DF8=list(range(nb_led[2], nb_led[3]+1))\n",
    "BF11=BF3+DF8\n",
    "ALL=list(range(0,35))\n",
    "LEDS=[False,BF3,DF8,BF11,ALL,'multi_led']\n",
    "disks=[\"HDD\",\"SSD\"]\n",
    "archis=['resnet_101','densenet_201','resnet_152']\n",
    "\n",
    "#%% parametres réglable\n",
    "#dimension d'entrée\n",
    "input_shape=(84,84,3)\n",
    "#pas d'apprentissage\n",
    "lr=0.00001\n",
    "#momentumm\n",
    "momentum=0.9\n",
    "#nombre d'entrainement\n",
    "nb_train=300\n",
    "#taille du batch\n",
    "batch_size=64\n",
    "#séparation train val et test\n",
    "nb_split=0.8\n",
    "#mélange de train et val\n",
    "shuffle=True\n",
    "#séparation train et val\n",
    "nb_val=0.8\n",
    "#nombre de classe\n",
    "num_cla=2\n",
    "#choix de la répartition\n",
    "dataset=datasets[3]\n",
    "#choix de pré entrainement\n",
    "pre_train=pre_trains[0]\n",
    "#couleur\n",
    "color=colors[1]\n",
    "#tophat\n",
    "tophat = False\n",
    "#champs\n",
    "champ=champs[0]\n",
    "# tableau modèle ensemble/entrainement images\n",
    "LOAD = [False, False]\n",
    "SAVE = [True , True, False]\n",
    "#date load\n",
    "date_load='data/'\n",
    "LED=LEDS[0]\n",
    "disk=disks[1]\n",
    "archi=archis[2]\n",
    "\n",
    "#%%\n",
    "#tophat\n",
    "if tophat is True:\n",
    "    pre_train=pre_trains[1]\n",
    "    input_shape=(input_shape[0], input_shape[1],2)\n",
    "    if color ==colors[3]:\n",
    "        color=colors[1]\n",
    "        \n",
    "\n",
    "#save folder\n",
    "Folder_save =  \"/content/drive/My Drive/Stage/classification/save\"\n",
    "#date =time.localtime()\n",
    "#date_str = str(date[0])+'-'+str(date[1])+'-'+str(date[2])\n",
    "Folder_load = \"/content/drive/My Drive/Stage/classification/\" +date_load\n",
    "#Folder_save = Folder_save +date_str+'/'\n",
    "Folder_save=creation_folder(Folder_save)\n",
    "    \n",
    "if LED is False:\n",
    "    input_shape=(84,84,3)\n",
    "elif type(LED) is str:\n",
    "    pre_train=pre_trains[1]\n",
    "else:\n",
    "    input_shape = (84,84,len(LED))\n",
    "    pre_train = pre_trains[1]\n",
    "    \n",
    "#model_convolution resnet_152 300 epochs combo dataset imagenet G RAW_0    \n",
    "#name\n",
    "if pre_train is None:\n",
    "    name=archi+' '+str(nb_train)+' epochs '+dataset+' dataset ' + ' imagenet '+color+' '+champ\n",
    "else:\n",
    "    name=archi+' '+str(nb_train)+' epochs '+dataset+' dataset '+pre_train+' '+color+' '+champ\n",
    "#%% model\n",
    "\n",
    "if LOAD[0]:\n",
    "#use LoadModel à la place\n",
    "    try :\n",
    "        model_01=LoadModel(Folder_load+'modele/model_convolution'+name)\n",
    "    except:\n",
    "        epoch_max=0\n",
    "        model_names=os.listdir(Folder_load+'checkpointer/')\n",
    "        for model_name in model_names:\n",
    "            epoch=model_name[model_name.find(\"epoch=\")+6:model_name.find(\"-val\")]\n",
    "            if int(epoch)>epoch_max:\n",
    "                epoch_max=int(epoch)\n",
    "                model_name_max=model_name\n",
    "        model_01=load_model(Folder_load+'checkpointer/'+model_name_max)\n",
    "        print(\"model load with :\",epoch_max)\n",
    "            \n",
    "else:\n",
    "    model_01 = create_model(archi, input_shape, num_cla, pre_train=pre_train)\n",
    "    \n",
    "    #https://towardsdatascience.com/understanding-and-coding-a-resnet-in-keras-446d7ff84d33\n",
    "\n",
    "    model_01.name=name\n",
    "\n",
    "#%%\n",
    "#extract data\n",
    "if LOAD[1]:\n",
    "    partition = mio.loadmat(Folder_load+'ensemble/'+model_01.name+' partition.mat')\n",
    "    for part in [\"train\",\"valid\",\"test\"]:\n",
    "        lenght=len(partition[part])\n",
    "        partition[part]=list(partition[part])\n",
    "        for indice,image_name in enumerate(partition[part]):\n",
    "            partition[part][indice]=image_name[:image_name.find(\".\")+4]\n",
    "            if len(partition[part][indice])<10:\n",
    "                print(image_name)\n",
    "    weights  = partition[\"weights\"]\n",
    "    \n",
    "else:\n",
    "    if dataset !=datasets[2]:\n",
    "        if disk==\"HDD\":\n",
    "            travel_healthy=\"/content/drive/My Drive/Stage/fpm_84/\"\n",
    "            travel_infected=\"/content/drive/My Drive/Stage/fpm_84/\"\n",
    "        elif disk=='SSD':\n",
    "            travel_healthy=\"/content/drive/My Drive/Stage/fpm_84/\"\n",
    "            travel_infected=\"/content/drive/My Drive/Stage/fpm_84/\"\n",
    "        \n",
    "        #infected extract\n",
    "        image_infected=[]\n",
    "        #travel_input_KPJ0=travel_infected+patients[1]+'/'+color+'/'+champ+'/'+'infected/'\n",
    "        travel_input_CAT01 = travel_infected+patients[0]\n",
    "        #folders_KPJ0 = os.listdir(travel_input_KPJ0)\n",
    "        folders_CAT01 = os.listdir(travel_input_CAT01)\n",
    "        # for i in range(len(folders_KPJ0)):    \n",
    "        #     folders_KPJ0[i] = travel_input_KPJ0+folders_KPJ0[i]\n",
    "        for i in range(len(folders_CAT01)):    \n",
    "            folders_CAT01[i] = travel_input_CAT01+folders_CAT01[i]\n",
    "        folders = folders_CAT01#+folders_KPJ0\n",
    "        random.shuffle(folders)\n",
    "        \n",
    "        for folder in folders:\n",
    "            if '_tophat' in folder:\n",
    "                pass\n",
    "            else:\n",
    "                files= os.listdir(folder)\n",
    "                n = 0\n",
    "                for file in files:\n",
    "                    if 'infected' in file:\n",
    "                        if '_inten.png' in file:\n",
    "                          if 'aug_29' not in file:\n",
    "                            if 'aug_28' not in file:\n",
    "                              image_infected.append(folder+'/'+file)\n",
    "                              n=n+1\n",
    "                print(folder)\n",
    "                print(n)\n",
    "        print(image_infected)\n",
    "        print(len(image_infected))\n",
    "        nb_cells_infected=len(image_infected)\n",
    "        #healthy extract\n",
    "        nb_cells_healthy=nb_cells_infected\n",
    "        image_healthy=[]\n",
    "        #on prend toute les cellules de CAT01\n",
    "        patient=patients[0]\n",
    "        travel_input=\"/content/drive/My Drive/Stage/fpm_84/\"+patients[0]\n",
    "        folders = os.listdir(travel_input)\n",
    "        for folder in folders:\n",
    "            if not '_tophat' in folder:\n",
    "                files = os.listdir(travel_input+folder)\n",
    "                for file in files:\n",
    "                    if 'healthy' in file:\n",
    "                      if '_inten.png' in file:\n",
    "                        image_healthy.append(travel_input+folder+'/'+file)\n",
    "\n",
    "        print(len(image_healthy))                   \n",
    "        nb_cells_healthy_rest = nb_cells_healthy-len(image_healthy)\n",
    "        nb_cells_healthy_part=int(nb_cells_healthy_rest/3)\n",
    "        cells_choice=[]\n",
    "        for patient in patients[1:]:\n",
    "            cells_patient=[]\n",
    "            travel_input=\"/content/drive/My Drive/Stage/fpm_84/\"+patients[0]\n",
    "            folders = os.listdir(travel_input)\n",
    "            for folder in folders:\n",
    "                if '_tophat' in folder :\n",
    "                    pass\n",
    "                else:\n",
    "                    files= os.listdir(travel_input+folder)\n",
    "                    for file in files:\n",
    "                        if 'healthy' in file:\n",
    "                          if '_inten.png' in file:\n",
    "                            cells_patient.append(travel_input+folder+'/'+file)\n",
    "            cells_choice.append(cells_patient)\n",
    "        \n",
    "        # for cells_patient in cells_choice:\n",
    "        #     if len(cells_patient)==0:\n",
    "        #         pass\n",
    "        #     else:\n",
    "        #         cells=random.sample(cells_patient, nb_cells_healthy_part)\n",
    "        #         image_healthy=image_healthy+cells\n",
    "        \n",
    "        #différente répartition\n",
    "        if dataset == datasets[0]:\n",
    "            image_total = image_healthy+image_infected\n",
    "            random.shuffle(image_total)\n",
    "            \n",
    "            image_train=image_total[:int(len(image_total)*nb_split*nb_val)]\n",
    "            image_val  =image_total[int(len(image_total)*nb_split*nb_val):int(len(image_total)*nb_split)]\n",
    "            image_test =image_total[int(len(image_total)*nb_split):]\n",
    "            \n",
    "            del(image_total)\n",
    "            \n",
    "        elif dataset == datasets[1] or dataset ==datasets[3]:\n",
    "            point_depart = random.randint(0,nb_cells_infected-1)\n",
    "            \n",
    "            if point_depart+len(image_infected)*nb_split*nb_val < len(image_infected) :\n",
    "                #cas ou le point de départ est avant les 34 premier %\n",
    "                image_train=image_infected[point_depart:point_depart+int(len(image_infected)*nb_split*nb_val)]\n",
    "                \n",
    "                if point_depart+len(image_infected)*nb_split< len(image_infected) :\n",
    "                    #cas ou le point de départ est avant les 20 premier %\n",
    "                    image_val=image_infected[point_depart+int(len(image_infected)*nb_split*nb_val):\\\n",
    "                                             point_depart+\\\n",
    "                                            int(len(image_infected)*nb_split)]\n",
    "                    \n",
    "                    if point_depart+len(image_infected)== len(image_infected):\n",
    "                        #cas ou le point de départ est 0\n",
    "                        image_test=image_infected[point_depart+int(len(image_infected)*nb_split):]\n",
    "                        \n",
    "                    else:\n",
    "                        image_test = image_infected[point_depart+int(len(image_infected)*nb_split):]\n",
    "                        nb_cell_case = len(image_test)\n",
    "                        nb_cell_rest = int(len(image_infected)*(1-nb_split))\n",
    "                        image_test = image_test + image_infected[:nb_cell_rest]\n",
    "                    \n",
    "                else:\n",
    "                    image_val=image_infected[point_depart+int(len(image_infected)*nb_split*nb_val):]\n",
    "                    nb_cell_case = len(image_val)\n",
    "                    nb_cell_rest = int(len(image_infected)*(1-nb_split)*(1-nb_val))\n",
    "                    image_val = image_val + image_infected[:nb_cell_rest]\n",
    "                    \n",
    "                    image_test = image_infected[nb_cell_rest:int(len(image_infected)*(1-nb_split))]\n",
    "                \n",
    "            else:\n",
    "                #cas ou le point de départ est au dela des 34 premier %\n",
    "                image_train=image_infected[point_depart:]\n",
    "                nb_cell_case=len(image_train)\n",
    "                nb_cell_rest=int(len(image_infected)*nb_split*nb_val-nb_cell_case)\n",
    "                image_train=image_train+image_infected[:nb_cell_rest]\n",
    "                \n",
    "                image_val  =image_infected[nb_cell_rest:nb_cell_rest+int(len(image_infected)*(nb_split)*\\\n",
    "                                                                         (1-nb_val))]\n",
    "                image_test =image_infected[nb_cell_rest+int(len(image_infected)*(nb_split)*(1-nb_val)):\\\n",
    "                                        nb_cell_rest+int(len(image_infected)*(nb_split)*(1-nb_val))+\\\n",
    "                                        int(len(image_infected)*(1-nb_split))]\n",
    "            if dataset == datasets[1]:    \n",
    "                image_train = image_train + image_healthy[:len(image_train)]\n",
    "                image_val   = image_train + image_healthy[len(image_train):len(image_train)+len(image_val)]\n",
    "                image_test  = image_test  + image_healthy[-len(image_test):]\n",
    "                \n",
    "                random.shuffle(image_train)\n",
    "                random.shuffle(image_val)\n",
    "                random.shuffle(image_test)\n",
    "            \n",
    "        weights=[1,1]\n",
    "                \n",
    "    if dataset==datasets[2] or dataset== datasets[3]:\n",
    "        if disk==\"HDD\":\n",
    "            travel=\"/content/drive/My Drive/Stage/fpm_84/\"\n",
    "        elif disk==\"SSD\":\n",
    "            travel=\"/content/drive/My Drive/Stage/fpm_84/\"\n",
    " \n",
    "        if dataset == datasets[2]:\n",
    "            #infected extract\n",
    "            image_infected=[]\n",
    "            for patient in patients[:2]:\n",
    "                travel_input=travel+patient+'/'+color+'/'+champ+'/'\n",
    "                folders = os.listdir(travel_input)\n",
    "                for folder in folders:\n",
    "                    if '_tophat' in folder:\n",
    "                        pass\n",
    "                    else:\n",
    "                        files= os.listdir(travel_input+folder)\n",
    "                        for file in files:\n",
    "                            if 'infected' in file:\n",
    "                              if '_inten.png' in file:\n",
    "                                image_infected.append(travel_input+folder+'/'+file)\n",
    "                                \n",
    "            nb_cells_infected=len(image_infected)\n",
    "        #healthy extract\n",
    "        nb_cells_healthy=nb_cells_infected\n",
    "        image_healthy=[]\n",
    "        #on prend toute les cellules\n",
    "        for patient in patients[:1]:\n",
    "            cells_patient=[]\n",
    "            travel_input=\"/content/drive/My Drive/Stage/fpm_84/\"+patient\n",
    "            folders = os.listdir(travel_input)\n",
    "            for folder in folders:\n",
    "                if '_tophat' in folder:\n",
    "                    pass\n",
    "                else:\n",
    "                    files= os.listdir(travel_input+folder)\n",
    "                    for file in files:\n",
    "                        if 'healthy' in file:\n",
    "                          if '_inten.png' in file:\n",
    "                            image_healthy.append(travel_input+folder+'/'+file)\n",
    "    \n",
    "        #constitution des ensembles train     \n",
    "        if dataset == datasets[2] :           \n",
    "            random.shuffle(image_infected)\n",
    "        random.shuffle(image_healthy)\n",
    "        image_train=image_infected[:int(len(image_infected)*nb_split*nb_val)]+\\\n",
    "                    image_healthy[:int(len(image_healthy)*nb_split*nb_val)]\n",
    "        \n",
    "        image_val=image_infected[int(len(image_infected)*nb_split*nb_val):int(len(image_infected)*nb_split)]+\\\n",
    "                  image_healthy[int(len(image_healthy)*nb_split*nb_val):int(len(image_healthy)*nb_split)]                \n",
    "        \n",
    "        image_test=image_infected[int(len(image_infected)*nb_split):]+image_healthy[int(len(image_healthy)*nb_split):]\n",
    "        \n",
    "        random.shuffle(image_train)\n",
    "        random.shuffle(image_val)\n",
    "        random.shuffle(image_test)\n",
    "        \n",
    "        \n",
    "        Y_w=[]\n",
    "        len_infec = 0 \n",
    "        len_healt = 0 \n",
    "        for image in image_infected+image_healthy:\n",
    "            trav, image_name = os.path.split(image)\n",
    "            if 'healthy' in image_name:\n",
    "                len_healt = len_healt + 1\n",
    "                Y_w.append(0)\n",
    "            elif 'infected' in image_name:\n",
    "                len_infec = len_infec + 1\n",
    "                Y_w.append(1)\n",
    "            else:\n",
    "                print(\"problème label\")\n",
    "        Y_w = np.array(Y_w)\n",
    "            \n",
    "        weights = class_weight.compute_class_weight('balanced', np.unique(Y_w), Y_w)\n",
    "        \n",
    "        \n",
    "        print(Y_w)\n",
    "        print(len_healt)\n",
    "        print(len_infec)\n",
    "        print(weights)\n",
    "        \n",
    "    del(image_infected)\n",
    "    del(image_healthy)\n",
    " \n",
    "    #répartition des chemin en ensemble \n",
    "    \n",
    "    partition={'train':[],'valid':[],'test':[]}\n",
    "    partition['train'] = image_train\n",
    "    partition['valid'] = image_val\n",
    "    partition['test' ] = image_test\n",
    "    partition['weights'] = weights\n",
    "        \n",
    "    del(image_train)\n",
    "    del(image_val)\n",
    "    del(image_test)\n",
    "\n",
    "if SAVE[1]:\n",
    "    save_data(Folder_save+'ensemble/'+model_01.name+' partition.mat', partition)\n",
    "#    if not os.path.exists(Folder_save+'ensemble/'):\n",
    "#        os.mkdir(Folder_save+'ensemble/')\n",
    "#    mio.savemat(Folder_save+'ensemble/'+model_01.name+' partition.mat',partition)\n",
    "#%%\n",
    "#Generators \n",
    "if tophat:\n",
    "    training_generator   = DataGeneratorTopHat(partition[\"train\"], batch_size=batch_size, dim=input_shape[:2], \n",
    "                                       n_channels=input_shape[2], n_classes=num_cla, shuffle=shuffle)\n",
    "    validation_generator = DataGeneratorTopHat(partition[\"valid\"], batch_size=batch_size, dim=input_shape[:2], \n",
    "                                       n_channels=input_shape[2], n_classes=num_cla, shuffle=shuffle)\n",
    "    testing_generator    = DataGeneratorTopHat(partition[\"test\" ], batch_size=batch_size, dim=input_shape[:2], \n",
    "                                       n_channels=input_shape[2], n_classes=num_cla, shuffle=False)\n",
    "    \n",
    "else :    \n",
    "    training_generator   = DataGenerator(partition[\"train\"], batch_size=batch_size, dim=input_shape[:2], \n",
    "                                       n_channels=input_shape[2], n_classes=num_cla, shuffle=shuffle, LEDS=LED)\n",
    "    validation_generator = DataGenerator(partition[\"valid\"], batch_size=batch_size, dim=input_shape[:2], \n",
    "                                       n_channels=input_shape[2], n_classes=num_cla, shuffle=shuffle, LEDS=LED)\n",
    "    testing_generator    = DataGenerator(partition[\"test\" ], batch_size=batch_size, dim=input_shape[:2], \n",
    "                                       n_channels=input_shape[2], n_classes=num_cla, shuffle=False, LEDS=LED)\n",
    "\n",
    "#%% train\n",
    "opt= Adam(lr, momentum)\n",
    "model_01.name=\"modelo\"\n",
    "model_01.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "# model_01.summary()\n",
    "\n",
    "#model_01.summary()\n",
    "\n",
    "if LOAD[0]:\n",
    "    history1=model_01.history\n",
    "    if SAVE[1]: \n",
    "        if not os.path.exists(Folder_save+'checkpointer/'):\n",
    "            os.mkdir(Folder_save+'checkpointer/')\n",
    "        if not os.path.exists(Folder_save+'history/'):\n",
    "            os.mkdir(Folder_save+'history/')            \n",
    "        trainpath=Folder_save+'history/'+archi+' train.log'\n",
    "        csv_logger = CSVLogger(trainpath)\n",
    "        modelpath=Folder_save+'checkpointer/'+archi+'-epoch={epoch:03d}-val_acc={val_acc:.2f}.hdf5'\n",
    "        checkpointer = ModelCheckpoint(filepath=modelpath, monitor=\"val_acc\", verbose=1, \n",
    "                                       save_best_only=False, save_weights_only=False, period=10)\n",
    "        history = model_01.fit_generator(generator=training_generator, validation_data=\\\n",
    "                                         validation_generator, epochs=nb_train, verbose=True,\n",
    "                                         class_weight=weights,callbacks=[checkpointer,csv_logger])\n",
    "    else:\n",
    "        history = model_01.fit_generator(generator=training_generator, \n",
    "                                   validation_data=validation_generator, \n",
    "                                   epochs=nb_train, verbose=True,class_weight=weights)\n",
    "        for key in history.history.keys():\n",
    "            history.history[key]=history1[key]+history.history[key]\n",
    "else:\n",
    "    if SAVE[1]: \n",
    "        if not os.path.exists(Folder_save+'checkpointer/'):\n",
    "            os.mkdir(Folder_save+'checkpointer/')\n",
    "        if not os.path.exists(Folder_save+'history/'):\n",
    "            os.mkdir(Folder_save+'history/')\n",
    "            \n",
    "        trainpath=Folder_save+'history/'+archi+' train.log'\n",
    "        csv_logger = CSVLogger(trainpath)\n",
    "        modelpath=Folder_save+'checkpointer/'+archi+' epoch={epoch:03d}-val_acc={val_accuracy:.2f}.hdf5'\n",
    "        checkpointer = ModelCheckpoint(filepath=modelpath, monitor=\"val_accuracy\", verbose=1, \n",
    "                                       save_best_only=False, save_weights_only=False, period=10)\n",
    "        history = model_01.fit_generator(generator=training_generator, validation_data=\\\n",
    "                                         validation_generator, epochs=nb_train, verbose=True,\n",
    "                                         class_weight=weights,callbacks=[checkpointer,csv_logger])\n",
    "    \n",
    "    else:\n",
    "        history = model_01.fit_generator(generator=training_generator, validation_data=\\\n",
    "                                         validation_generator, epochs=nb_train, verbose=True,\n",
    "                                         class_weight=weights)\n",
    "        \n",
    "##%% bidouyage\n",
    "#from keras.callbacks import History\n",
    "#import pandas as pd\n",
    "#histoire=History()\n",
    "#setattr(histoire, 'epoch', np.array(range(160)))\n",
    "#histoiri=pd.read_csv(Folder_load+'history/resnet_101 train.log')\n",
    "#keys=history.history.keys()\n",
    "#dico_histoire={}\n",
    "#for key in keys:\n",
    "#    dico_histoire[key]=list(histoiri[key])[:80]\n",
    "#setattr(histoire, 'history', dico_histoire)\n",
    "#for key in keys:\n",
    "#    histoire.history[key]+=history.history[key]\n",
    "#histoire.set_params(params)\n",
    "#history1=history\n",
    "#history=histoire\n",
    "\n",
    "#%% visualisation\n",
    "#plt.figure()\n",
    "#plt.subplot(1,2,1)\n",
    "#plt.plot(history.history[\"acc\"])\n",
    "#plt.plot(history.history[\"val_acc\"])\n",
    "#plt.title(\"model accuracy\")\n",
    "#plt.ylabel(\"accuracy\")\n",
    "#plt.xlabel(\"epoch\")\n",
    "#plt.legend([\"train\",\"validation\"],loc='upper left')\n",
    "#plt.show()\n",
    "#\n",
    "#plt.subplot(1,2,2)\n",
    "#plt.plot(history.history['loss'])\n",
    "#plt.plot(history.history['val_loss'])\n",
    "#plt.title(\"model loss\")\n",
    "#plt.ylabel(\"loss\")\n",
    "#plt.xlabel(\"epoch\")\n",
    "#plt.legend(['train','validation'], loc='upper left')\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1066,
     "status": "ok",
     "timestamp": 1600782245099,
     "user": {
      "displayName": "Juliana Damurie",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgflBzYptWKQ0dbJgh2lCPGPTscmRPFiDOL4HXGBw=s64",
      "userId": "11274300898498138619"
     },
     "user_tz": -120
    },
    "id": "GlXaxsvx_lBN"
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "def plot_train(model, title='train'):\n",
    "    \"\"\"\n",
    "    prend en entrée un modèle entrainé ou l'historique et trace en fonction de ça.\n",
    "    les courbes d'accuracy et d'erreur. \n",
    "    \n",
    "    amélioration possible ne pas fixé en dur les métrics comme c'est fait mais \n",
    "    récupérer les clés du dictionnaire history.\n",
    "    \"\"\"\n",
    "    if type(model)==Model:\n",
    "        plt.figure(figsize=(30,10))\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(model.history.history[\"accuracy\"])\n",
    "        plt.plot(model.history.history[\"val_accuracy\"])\n",
    "        plt.title(\"model accuracy\")\n",
    "        plt.ylabel(\"accuracy\")\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.legend([\"train\",\"validation\"],loc='lower right')\n",
    "        \n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(model.history.history['loss'])\n",
    "        plt.plot(model.history.history['val_loss'])\n",
    "        plt.title(\"model loss\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.legend(['train','validation'], loc='upper left')\n",
    "        plt.show()\n",
    "    elif type(model)==History:\n",
    "        plt.figure(figsize=(20,30))\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(model.history[\"acc\"])\n",
    "        plt.plot(model.history[\"val_acc\"])\n",
    "        plt.title(\"model accuracy\")\n",
    "        plt.ylabel(\"accuracy\")\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.legend([\"train\",\"validation\"],loc='upper left')\n",
    "\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(model.history['loss'])\n",
    "        plt.plot(model.history['val_loss'])\n",
    "        plt.title(\"model loss\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.legend(['train','validation'], loc='upper left')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 952
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3229,
     "status": "error",
     "timestamp": 1600782248147,
     "user": {
      "displayName": "Juliana Damurie",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgflBzYptWKQ0dbJgh2lCPGPTscmRPFiDOL4HXGBw=s64",
      "userId": "11274300898498138619"
     },
     "user_tz": -120
    },
    "id": "EsTkjWduZ_Jn",
    "outputId": "5082b11b-709d-4319-a062-55d65127afae"
   },
   "outputs": [],
   "source": [
    "LED = False \n",
    "\n",
    "plot_train(model_01)\n",
    "\n",
    "if SAVE[0]:\n",
    "    SaveModel(model_01, where=Folder_save)\n",
    "#%%\n",
    "#évaluation\n",
    "scores = model_01.evaluate_generator(generator=testing_generator)\n",
    "print(\"%s: %.2f%%\" % (model_01.metrics_names[0], scores[0]*100))\n",
    "print(\"%s: %.2f%%\" % (model_01.metrics_names[1], scores[1]*100))\n",
    "\n",
    "Y_pred = model_01.predict_generator(generator=testing_generator)\n",
    "Y_pred = np.argmax(Y_pred, axis=1)\n",
    "Y_test = get_labels(partition['test'])[:len(Y_pred)]\n",
    "Y_test = Y_test.reshape([len(Y_test),])\n",
    "Y_test = np.array(Y_test, dtype='uint64')\n",
    "\n",
    "\n",
    "matrix = confusion_matrix(Y_test, Y_pred)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(matrix)\n",
    "plt.title(\"matrice confusion\")\n",
    "for i in range(matrix.shape[0]):\n",
    "    for j in range(matrix.shape[1]):\n",
    "        plt.text(i,j,str(matrix[j,i]))\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "plt.xlabel(\"prediction\")\n",
    "plt.ylabel(\"reality\")\n",
    "plt.show()\n",
    "\n",
    "#%%visualisation error\n",
    "\n",
    "f_neg = np.where(Y_test > Y_pred)\n",
    "f_pos = np.where(Y_test < Y_pred)\n",
    "part=np.array(partition['test'])\n",
    "image_neg_name = part[f_neg]\n",
    "image_pos_name = part[f_pos]\n",
    "\n",
    "image_neg = []\n",
    "image_pos = []\n",
    "for i in range(len(image_neg_name)):\n",
    "    image_neg.append(imread(image_neg_name[i]))\n",
    "for i in range(len(image_pos_name)):    \n",
    "    image_pos.append(imread(image_pos_name[i]))\n",
    "\n",
    "\n",
    "    \n",
    "plt.figure(figsize=(20,20))\n",
    "for j in range(min(5,len(image_neg))):\n",
    "    plt.subplot(2,5,j+1)\n",
    "    if LED:\n",
    "        plt.imshow(image_neg[j][0,:,:], cmap='gray')\n",
    "    else:\n",
    "        plt.imshow(image_neg[j], cmap='gray')\n",
    "    plt.title('faux négatif')\n",
    "    \n",
    "for j in range(5,min(10,5+len(image_pos))):\n",
    "    plt.subplot(2,5,j+1)\n",
    "    if LED:\n",
    "        plt.imshow(image_pos[j-5][0,:,:], cmap='gray')\n",
    "    else:\n",
    "        plt.imshow(image_pos[j-5], cmap='gray')\n",
    "    plt.title(\"faux positif\")\n",
    "plt.show()\n",
    "\n",
    "if SAVE[2]:\n",
    "    travel_output=Folder_save+'image/'\n",
    "    if not os.path.exists(travel_output):\n",
    "        os.mkdir(travel_output)\n",
    "    travel_output=travel_output+model_01.name+'/'\n",
    "    if not os.path.exists(travel_output):\n",
    "        os.mkdir(travel_output)\n",
    "    #faux négatif\n",
    "    travel_output_neg=travel_output+'/faux_neg/'\n",
    "    if not os.path.exists(travel_output_neg):\n",
    "        os.mkdir(travel_output_neg)\n",
    "    for i,image in enumerate(image_neg):\n",
    "        traveling, name_image = os.path.split(image_neg_name[i])\n",
    "        traveling, folder = os.path.split(traveling)\n",
    "        traveling, stat = os.path.split(traveling)\n",
    "        traveling, champ = os.path.split(traveling)\n",
    "        traveling, colori = os.path.split(traveling)\n",
    "        traveling, patient = os.path.split(traveling)\n",
    "        title=patient+'/'\n",
    "        if not os.path.exists(travel_output_neg+title):\n",
    "            os.mkdir(travel_output_neg+title)\n",
    "        title=title+colori+'/'\n",
    "        if not os.path.exists(travel_output_neg+title):\n",
    "            os.mkdir(travel_output_neg+title)\n",
    "        title=title+champ+'/'\n",
    "        if not os.path.exists(travel_output_neg+title):\n",
    "            os.mkdir(travel_output_neg+title)\n",
    "        title=title+folder+'/'\n",
    "        if not os.path.exists(travel_output_neg+title):\n",
    "            os.mkdir(travel_output_neg+title)\n",
    "        title=title+name_image\n",
    "        imsave(travel_output_neg+title, image)\n",
    "        \n",
    "    #faux positif\n",
    "    travel_output_pos=travel_output+'/faux_pos/'\n",
    "    if not os.path.exists(travel_output_pos):\n",
    "        os.mkdir(travel_output_pos)\n",
    "    for i,image in enumerate(image_pos):\n",
    "        traveling, name_image = os.path.split(image_pos_name[i])\n",
    "        traveling, folder = os.path.split(traveling)\n",
    "        traveling, stat = os.path.split(traveling)\n",
    "        traveling, champ = os.path.split(traveling)\n",
    "        traveling, colori = os.path.split(traveling)\n",
    "        traveling, patient = os.path.split(traveling)\n",
    "        title=patient+'/'\n",
    "        if not os.path.exists(travel_output_pos+title):\n",
    "            os.mkdir(travel_output_pos+title)\n",
    "        title=title+color+'/'\n",
    "        if not os.path.exists(travel_output_pos+title):\n",
    "            os.mkdir(travel_output_pos+title)\n",
    "        title=title+colori+'/'\n",
    "        if not os.path.exists(travel_output_pos+title):\n",
    "            os.mkdir(travel_output_pos+title)    \n",
    "        title=title+color+'/'\n",
    "        if not os.path.exists(travel_output_pos+title):\n",
    "            os.mkdir(travel_output_pos+title)\n",
    "        title=title+name_image\n",
    "        imsave(travel_output_pos+title, image)\n",
    "# #%%\n",
    "fin=time.time()\n",
    "#print(datetime.timedelta(0,fin-debut))\n",
    "plot_time(debut, fin, title='process complet in:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11,
     "status": "aborted",
     "timestamp": 1600782248149,
     "user": {
      "displayName": "Juliana Damurie",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgflBzYptWKQ0dbJgh2lCPGPTscmRPFiDOL4HXGBw=s64",
      "userId": "11274300898498138619"
     },
     "user_tz": -120
    },
    "id": "Ig-PaAnp12-Z"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib.pyplot import imread, imshow, subplots, show\n",
    "\n",
    "\n",
    "def plot(data_generator):\n",
    "    \"\"\"\n",
    "    Plots 4 images generated by an object of the ImageDataGenerator class.\n",
    "    \"\"\"\n",
    "    data_generator.fit(images)\n",
    "    image_iterator = data_generator.flow(images)\n",
    "    \n",
    "    # Plot the images given by the iterator\n",
    "    fig, rows = subplots(nrows=1, ncols=4, figsize=(18,18))\n",
    "    for row in rows:\n",
    "        row.imshow(image_iterator.next()[0].astype('int'))\n",
    "        row.axis('off')\n",
    "    show()\n",
    "    \n",
    "image = imread(\"image.jpeg\")\n",
    "\n",
    "# Creating a dataset which contains just one image.\n",
    "images = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "\n",
    "imshow(images[0])\n",
    "show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
